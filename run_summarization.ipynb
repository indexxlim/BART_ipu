{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "926116a2-ac08-459a-8f43-a50e0b893fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import nltk  # Here to have a nice missing dependency error message early on\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import transformers\n",
    "from filelock import FileLock\n",
    "from optimum.graphcore import IPUConfig, IPUSeq2SeqTrainer\n",
    "from optimum.graphcore import IPUSeq2SeqTrainingArguments as Seq2SeqTrainingArguments\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    MBart50Tokenizer,\n",
    "    MBart50TokenizerFast,\n",
    "    MBartTokenizer,\n",
    "    MBartTokenizerFast,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version as tf_check_min_version\n",
    "from transformers.utils import is_offline_mode\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "import yaml\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954410b-2ed9-4732-8066-f78a63293554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f2165a-31c1-43eb-b4af-23d71f1da6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "__version__ = \"0.2.4.dev\"\n",
    "\n",
    "def check_min_version(min_version):\n",
    "    if version.parse(__version__) < version.parse(min_version):\n",
    "        if \"dev\" in min_version:\n",
    "            error_message = \"This example requires a source install from HuggingFace Optimum-Graphcore\"\n",
    "        else:\n",
    "            error_message = f\"This example requires a minimum version of {min_version},\"\n",
    "        error_message += f\" but the version found is {__version__}.\\n\"\n",
    "        raise ImportError(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f1a6b-c860-4f06-95c2-12294b17567c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ecf8c-6a79-41c5-89ca-4f7485a77dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fcc829b-9352-42c4-b92c-87a087ef3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "#tf_check_min_version(\"4.19.0.dev0\")\n",
    "\n",
    "# Will error if the minimal version of Optimum Graphcore is not installed. Remove at your own risks.\n",
    "check_min_version(\"0.2.4.dev\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/summarization/requirements.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f80d099a-1341-432e-a047-5c8ef41ac272",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb866fe-5b62-4b7e-a081-bcc07fdecfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    if is_offline_mode():\n",
    "        raise LookupError(\n",
    "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
    "        )\n",
    "    with FileLock(\".lock\") as lock:\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# A list of all multilingual tokenizer which require lang attribute.\n",
    "MULTILINGUAL_TOKENIZERS = [MBartTokenizer, MBartTokenizerFast, MBart50Tokenizer, MBart50TokenizerFast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d7e963f-f777-43a4-9174-47f60225bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summarization_name_mapping = {\n",
    "    \"amazon_reviews_multi\": (\"review_body\", \"review_title\"),\n",
    "    \"big_patent\": (\"description\", \"abstract\"),\n",
    "    \"cnn_dailymail\": (\"article\", \"highlights\"),\n",
    "    \"orange_sum\": (\"text\", \"summary\"),\n",
    "    \"pn_summary\": (\"article\", \"summary\"),\n",
    "    \"psc\": (\"extract_text\", \"summary_text\"),\n",
    "    \"samsum\": (\"dialogue\", \"summary\"),\n",
    "    \"thaisum\": (\"body\", \"summary\"),\n",
    "    \"xglue\": (\"news_body\", \"news_title\"),\n",
    "    \"xsum\": (\"document\", \"summary\"),\n",
    "    \"wiki_summary\": (\"article\", \"highlights\"),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7742d36c-52a8-4fb7-824c-7c964a30a393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eea6248-7af8-422a-a02e-0b95cfd319cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/27/2022 07:51:40 - INFO - __main__ - Training/evaluation parameters  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "log_level = -1 # training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "logger.info(f\"Training/evaluation parameters  \")\n",
    "\n",
    "source_prefix:'summarize: '\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4b78b-1a6d-4ecd-9ef0-595f7e049de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "73630f10-2dcd-4b62-a712-e83e1ff7e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "argus = {}\n",
    "with open('configuration/summrization.yaml') as f:\n",
    "    hparams = yaml.load_all(f, Loader=yaml.FullLoader)\n",
    "    for argu in hparams:\n",
    "        argus[list(argu.keys())[0]]=list(argu.values())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e58c9386-a817-447d-96b4-fb4426c9b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args, data_args, training_args = argus['ModelArguments'], argus['DataTrainingArguments'], argus['IPUSeq2SeqTrainingArguments']\n",
    "model_args, data_args = EasyDict(model_args), EasyDict(data_args)\n",
    "training_args = Seq2SeqTrainingArguments(**training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a1689-4ce3-4aaf-ad9d-38ec266f04c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "694cd942-fe6a-457f-a811-605b1ba08189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114fadf-4698-4c26-a0d2-911453dad3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0eca13-95f8-4b75-b0ba-06b3738da123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "242238ca-d745-4f3d-90d4-0fea06a28a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/27/2022 09:24:45 - DEBUG - datasets.load - Checking /root/.cache/huggingface/datasets/downloads/12660d351f66522858b33549dc51dcb9e2175c19106780ee8f29e3dc4af71195.ef876a56bbc40513db0e303f5b9afb0e07995a4dd89f5f5a9a339ffdb1da19a5.py for additional imports.\n",
      "04/27/2022 09:24:45 - DEBUG - datasets.utils.filelock - Attempting to acquire lock 139782530386912 on /root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail.lock\n",
      "04/27/2022 09:24:45 - DEBUG - datasets.utils.filelock - Lock 139782530386912 acquired on /root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail.lock\n",
      "04/27/2022 09:24:45 - DEBUG - datasets.utils.filelock - Attempting to release lock 139782530386912 on /root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail.lock\n",
      "04/27/2022 09:24:45 - DEBUG - datasets.utils.filelock - Lock 139782530386912 released on /root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail.lock\n",
      "04/27/2022 09:24:45 - DEBUG - datasets.load - Created importable dataset file at /root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cnn_dailymail.py\n",
      "04/27/2022 09:24:45 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234\n",
      "04/27/2022 09:24:45 - DEBUG - datasets.utils.filelock - Attempting to acquire lock 139782530385120 on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_cnn_dailymail_3.0.0_3.0.0_3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234.lock\n",
      "04/27/2022 09:24:45 - DEBUG - datasets.utils.filelock - Lock 139782530385120 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_cnn_dailymail_3.0.0_3.0.0_3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234.lock\n",
      "04/27/2022 09:24:45 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "04/27/2022 09:24:45 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234\n",
      "04/27/2022 09:24:45 - DEBUG - datasets.utils.filelock - Attempting to release lock 139782530385120 on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_cnn_dailymail_3.0.0_3.0.0_3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234.lock\n",
      "04/27/2022 09:24:45 - DEBUG - datasets.utils.filelock - Lock 139782530385120 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_cnn_dailymail_3.0.0_3.0.0_3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234.lock\n",
      "04/27/2022 09:24:45 - DEBUG - datasets.utils.filelock - Attempting to acquire lock 139782530763184 on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_cnn_dailymail_3.0.0_3.0.0_3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234.lock\n",
      "04/27/2022 09:24:45 - DEBUG - datasets.utils.filelock - Lock 139782530763184 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_cnn_dailymail_3.0.0_3.0.0_3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234.lock\n",
      "04/27/2022 09:24:45 - WARNING - datasets.builder - Reusing dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "04/27/2022 09:24:45 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234\n",
      "04/27/2022 09:24:45 - DEBUG - datasets.utils.filelock - Attempting to release lock 139782530763184 on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_cnn_dailymail_3.0.0_3.0.0_3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234.lock\n",
      "04/27/2022 09:24:45 - DEBUG - datasets.utils.filelock - Lock 139782530763184 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_cnn_dailymail_3.0.0_3.0.0_3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234.lock\n",
      "04/27/2022 09:24:45 - DEBUG - datasets.builder - Constructing Dataset for split train, validation, test, from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34477c09f03413885bedc31f56367c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set seed before initializing model.\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
    "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "# (the dataset will be downloaded automatically from the datasets Hub).\n",
    "#\n",
    "# For CSV/JSON files this script will use the first column for the full texts and the second column for the\n",
    "# summaries (unless you specify column names for this with the `text_column` and `summary_column` arguments).\n",
    "#\n",
    "# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if data_args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(\n",
    "        data_args.dataset_name,\n",
    "        data_args.dataset_config_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "else:\n",
    "    data_files = {}\n",
    "    if data_args.train_file is not None:\n",
    "        data_files[\"train\"] = data_args.train_file\n",
    "        extension = data_args.train_file.split(\".\")[-1]\n",
    "    if data_args.validation_file is not None:\n",
    "        data_files[\"validation\"] = data_args.validation_file\n",
    "        extension = data_args.validation_file.split(\".\")[-1]\n",
    "    if data_args.test_file is not None:\n",
    "        data_files[\"test\"] = data_args.test_file\n",
    "        extension = data_args.test_file.split(\".\")[-1]\n",
    "    raw_datasets = load_dataset(\n",
    "        extension,\n",
    "        data_files=data_files,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "93a4ac6f-448f-42f0-a30f-fcc44e9a18eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:654] 2022-04-27 09:24:46,303 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "[INFO|configuration_utils.py:690] 2022-04-27 09:24:46,305 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:344] 2022-04-27 09:24:48,987 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2022-04-27 09:24:49,787 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "[INFO|configuration_utils.py:690] 2022-04-27 09:24:49,789 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[DEBUG|tokenization_utils_base.py:1750] 2022-04-27 09:24:54,214 >> facebook/bart-base does not contain a file named https://huggingface.co/facebook/bart-base/resolve/main/added_tokens.json.\n",
      "[DEBUG|tokenization_utils_base.py:1750] 2022-04-27 09:24:55,019 >> facebook/bart-base does not contain a file named https://huggingface.co/facebook/bart-base/resolve/main/special_tokens_map.json.\n",
      "[DEBUG|tokenization_utils_base.py:1750] 2022-04-27 09:24:55,960 >> facebook/bart-base does not contain a file named https://huggingface.co/facebook/bart-base/resolve/main/tokenizer_config.json.\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-27 09:24:55,961 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/43978bdeaa326572886b44fcfed82f932f76571095ce31973e51c3da8ccade7f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-27 09:24:55,961 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/3c167ed8af56e6605eeb794b63a79d65d85e6708c9b04408d41946337030f5cd.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-27 09:24:55,961 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/a878fcd69bba037c9b1b227f4213579ae43d0aaa9374e167bc6c5f41b1cfeb30.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-27 09:24:55,961 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-27 09:24:55,962 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-27 09:24:55,962 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2022-04-27 09:24:56,797 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "[INFO|configuration_utils.py:690] 2022-04-27 09:24:56,798 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1772] 2022-04-27 09:24:57,702 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n",
      "[INFO|modeling_utils.py:2057] 2022-04-27 09:24:59,213 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:2066] 2022-04-27 09:24:59,214 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "# https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "ipu_config = IPUConfig.from_pretrained(\n",
    "    training_args.ipu_config_name if training_args.ipu_config_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast_tokenizer,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc2270-e879-4e12-999f-a3acb60a5df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a48f2fe0-67dd-4aaf-946d-34730973ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if model.config.decoder_start_token_id is None and isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):\n",
    "    if isinstance(tokenizer, MBartTokenizer):\n",
    "        model.config.decoder_start_token_id = tokenizer.lang_code_to_id[data_args.lang]\n",
    "    else:\n",
    "        model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids(data_args.lang)\n",
    "\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "\n",
    "if (\n",
    "    hasattr(model.config, \"max_position_embeddings\")\n",
    "    and model.config.max_position_embeddings < data_args.max_source_length\n",
    "):\n",
    "    if model_args.resize_position_embeddings is None:\n",
    "        logger.warning(\n",
    "            f\"Increasing the model's number of position embedding vectors from {model.config.max_position_embeddings} \"\n",
    "            f\"to {data_args.max_source_length}.\"\n",
    "        )\n",
    "        model.resize_position_embeddings(data_args.max_source_length)\n",
    "    elif model_args.resize_position_embeddings:\n",
    "        model.resize_position_embeddings(data_args.max_source_length)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"`--max_source_length` is set to {data_args.max_source_length}, but the model only has {model.config.max_position_embeddings}\"\n",
    "            f\" position encodings. Consider either reducing `--max_source_length` to {model.config.max_position_embeddings} or to automatically \"\n",
    "            \"resize the model's position encodings by passing `--resize_position_embeddings`.\"\n",
    "        )\n",
    "\n",
    "prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\"\n",
    "\n",
    "# Preprocessing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1406cc14-0511-4d23-9498-80ac33e99422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets.\n",
    "# We need to tokenize inputs and targets.\n",
    "if training_args.do_train:\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "elif training_args.do_eval:\n",
    "    column_names = raw_datasets[\"validation\"].column_names\n",
    "elif training_args.do_predict:\n",
    "    column_names = raw_datasets[\"test\"].column_names\n",
    "else:\n",
    "    logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n",
    "    raise\n",
    "\n",
    "if isinstance(tokenizer, tuple(MULTILINGUAL_TOKENIZERS)):\n",
    "    assert (\n",
    "        data_args.lang is not None\n",
    "    ), f\"{tokenizer.__class__.__name__} is a multilingual tokenizer which requires --lang argument\"\n",
    "\n",
    "    tokenizer.src_lang = data_args.lang\n",
    "    tokenizer.tgt_lang = data_args.lang\n",
    "\n",
    "    # For multilingual translation models like mBART-50 and M2M100 we need to force the target language token\n",
    "    # as the first generated token. We ask the user to explicitly provide this as --forced_bos_token argument.\n",
    "    forced_bos_token_id = (\n",
    "        tokenizer.lang_code_to_id[data_args.forced_bos_token] if data_args.forced_bos_token is not None else None\n",
    "    )\n",
    "    model.config.forced_bos_token_id = forced_bos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b2319f6c-1399-418c-95bf-4a1234d0dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the column names for input/target.\n",
    "dataset_columns = summarization_name_mapping.get(data_args.dataset_name, None)\n",
    "if data_args.text_column is None:\n",
    "    text_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
    "else:\n",
    "    text_column = data_args.text_column\n",
    "    if text_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--text_column' value '{data_args.text_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "if data_args.summary_column is None:\n",
    "    summary_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
    "else:\n",
    "    summary_column = data_args.summary_column\n",
    "    if summary_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--summary_column' value '{data_args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "\n",
    "# Temporarily set max_target_length for training.\n",
    "max_target_length = data_args.max_target_length\n",
    "padding = \"max_length\" if data_args.pad_to_max_length else False\n",
    "\n",
    "if training_args.label_smoothing_factor > 0 and not hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n",
    "    logger.warning(\n",
    "        \"label_smoothing is enabled but the `prepare_decoder_input_ids_from_labels` method is not defined for\"\n",
    "        f\"`{model.__class__.__name__}`. This will lead to loss being calculated twice and will take up more memory\"\n",
    "    )\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # remove pairs where at least one record is None\n",
    "\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(examples[text_column])):\n",
    "        if examples[text_column][i] is not None and examples[summary_column][i] is not None:\n",
    "            inputs.append(examples[text_column][i])\n",
    "            targets.append(examples[summary_column][i])\n",
    "\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c67f32d2-125e-4b7d-bee9-49a37ba1e147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/27/2022 09:24:59 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-a492a978dd4f6742.arrow\n"
     ]
    }
   ],
   "source": [
    "if training_args.do_train:\n",
    "    if \"train\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = raw_datasets[\"train\"]\n",
    "    if data_args.max_train_samples is not None:\n",
    "        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "        train_dataset = train_dataset.select(range(max_train_samples))\n",
    "    #with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
    "    train_dataset = train_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=data_args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on train dataset\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3a73a-c1a2-4f15-9b1e-55d3ee6970ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb9498-250a-489d-8852-09268837b69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a19efb-d00f-4882-9fc0-53844fd9e904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d904835-9452-4e5f-95e1-08499ad3d509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f3958-9841-49b6-bc54-2cf92c00319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 run_summarization.py --model_name_or_path t5-small --ipu_config_name Graphcore/t5-small-ipu --do_train --do_eval --dataset_name cnn_dailymail --dataset_config \"3.0.0\" --source_prefix \"summarize: \" --output_dir /tmp/tst-summarization --per_device_train_batch_size=2 --per_device_eval_batch_size=2 --pod_type pod8\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
